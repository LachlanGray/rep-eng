{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import load_model_and_tokenizer, get_submodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded microsoft/phi-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/phi-2\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "model.layers.0.self_attn.q_proj.bias\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "model.layers.0.self_attn.k_proj.bias\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "model.layers.0.self_attn.v_proj.bias\n",
      "model.layers.0.self_attn.dense.weight\n",
      "model.layers.0.self_attn.dense.bias\n",
      "model.layers.0.mlp.fc1.weight\n",
      "model.layers.0.mlp.fc1.bias\n",
      "model.layers.0.mlp.fc2.weight\n",
      "model.layers.0.mlp.fc2.bias\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.input_layernorm.bias\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "model.layers.1.self_attn.q_proj.bias\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "model.layers.1.self_attn.k_proj.bias\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "model.layers.1.self_attn.v_proj.bias\n",
      "model.layers.1.self_attn.dense.weight\n",
      "model.layers.1.self_attn.dense.bias\n",
      "model.layers.1.mlp.fc1.weight\n",
      "model.layers.1.mlp.fc1.bias\n",
      "model.layers.1.mlp.fc2.weight\n",
      "model.layers.1.mlp.fc2.bias\n",
      "model.layers.1.input_layernorm.weight\n",
      "model.layers.1.input_layernorm.bias\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "model.layers.2.self_attn.q_proj.bias\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "model.layers.2.self_attn.k_proj.bias\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "model.layers.2.self_attn.v_proj.bias\n",
      "model.layers.2.self_attn.dense.weight\n",
      "model.layers.2.self_attn.dense.bias\n",
      "model.layers.2.mlp.fc1.weight\n",
      "model.layers.2.mlp.fc1.bias\n",
      "model.layers.2.mlp.fc2.weight\n",
      "model.layers.2.mlp.fc2.bias\n",
      "model.layers.2.input_layernorm.weight\n",
      "model.layers.2.input_layernorm.bias\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "model.layers.3.self_attn.q_proj.bias\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "model.layers.3.self_attn.k_proj.bias\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "model.layers.3.self_attn.v_proj.bias\n",
      "model.layers.3.self_attn.dense.weight\n",
      "model.layers.3.self_attn.dense.bias\n",
      "model.layers.3.mlp.fc1.weight\n",
      "model.layers.3.mlp.fc1.bias\n",
      "model.layers.3.mlp.fc2.weight\n",
      "model.layers.3.mlp.fc2.bias\n",
      "model.layers.3.input_layernorm.weight\n",
      "model.layers.3.input_layernorm.bias\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "model.layers.4.self_attn.q_proj.bias\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "model.layers.4.self_attn.k_proj.bias\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "model.layers.4.self_attn.v_proj.bias\n",
      "model.layers.4.self_attn.dense.weight\n",
      "model.layers.4.self_attn.dense.bias\n",
      "model.layers.4.mlp.fc1.weight\n",
      "model.layers.4.mlp.fc1.bias\n",
      "model.layers.4.mlp.fc2.weight\n",
      "model.layers.4.mlp.fc2.bias\n",
      "model.layers.4.input_layernorm.weight\n",
      "model.layers.4.input_layernorm.bias\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "model.layers.5.self_attn.q_proj.bias\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "model.layers.5.self_attn.k_proj.bias\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "model.layers.5.self_attn.v_proj.bias\n",
      "model.layers.5.self_attn.dense.weight\n",
      "model.layers.5.self_attn.dense.bias\n",
      "model.layers.5.mlp.fc1.weight\n",
      "model.layers.5.mlp.fc1.bias\n",
      "model.layers.5.mlp.fc2.weight\n",
      "model.layers.5.mlp.fc2.bias\n",
      "model.layers.5.input_layernorm.weight\n",
      "model.layers.5.input_layernorm.bias\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "model.layers.6.self_attn.q_proj.bias\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "model.layers.6.self_attn.k_proj.bias\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "model.layers.6.self_attn.v_proj.bias\n",
      "model.layers.6.self_attn.dense.weight\n",
      "model.layers.6.self_attn.dense.bias\n",
      "model.layers.6.mlp.fc1.weight\n",
      "model.layers.6.mlp.fc1.bias\n",
      "model.layers.6.mlp.fc2.weight\n",
      "model.layers.6.mlp.fc2.bias\n",
      "model.layers.6.input_layernorm.weight\n",
      "model.layers.6.input_layernorm.bias\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "model.layers.7.self_attn.q_proj.bias\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "model.layers.7.self_attn.k_proj.bias\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "model.layers.7.self_attn.v_proj.bias\n",
      "model.layers.7.self_attn.dense.weight\n",
      "model.layers.7.self_attn.dense.bias\n",
      "model.layers.7.mlp.fc1.weight\n",
      "model.layers.7.mlp.fc1.bias\n",
      "model.layers.7.mlp.fc2.weight\n",
      "model.layers.7.mlp.fc2.bias\n",
      "model.layers.7.input_layernorm.weight\n",
      "model.layers.7.input_layernorm.bias\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "model.layers.8.self_attn.q_proj.bias\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "model.layers.8.self_attn.k_proj.bias\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "model.layers.8.self_attn.v_proj.bias\n",
      "model.layers.8.self_attn.dense.weight\n",
      "model.layers.8.self_attn.dense.bias\n",
      "model.layers.8.mlp.fc1.weight\n",
      "model.layers.8.mlp.fc1.bias\n",
      "model.layers.8.mlp.fc2.weight\n",
      "model.layers.8.mlp.fc2.bias\n",
      "model.layers.8.input_layernorm.weight\n",
      "model.layers.8.input_layernorm.bias\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "model.layers.9.self_attn.q_proj.bias\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "model.layers.9.self_attn.k_proj.bias\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "model.layers.9.self_attn.v_proj.bias\n",
      "model.layers.9.self_attn.dense.weight\n",
      "model.layers.9.self_attn.dense.bias\n",
      "model.layers.9.mlp.fc1.weight\n",
      "model.layers.9.mlp.fc1.bias\n",
      "model.layers.9.mlp.fc2.weight\n",
      "model.layers.9.mlp.fc2.bias\n",
      "model.layers.9.input_layernorm.weight\n",
      "model.layers.9.input_layernorm.bias\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "model.layers.10.self_attn.q_proj.bias\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "model.layers.10.self_attn.k_proj.bias\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "model.layers.10.self_attn.v_proj.bias\n",
      "model.layers.10.self_attn.dense.weight\n",
      "model.layers.10.self_attn.dense.bias\n",
      "model.layers.10.mlp.fc1.weight\n",
      "model.layers.10.mlp.fc1.bias\n",
      "model.layers.10.mlp.fc2.weight\n",
      "model.layers.10.mlp.fc2.bias\n",
      "model.layers.10.input_layernorm.weight\n",
      "model.layers.10.input_layernorm.bias\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "model.layers.11.self_attn.q_proj.bias\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "model.layers.11.self_attn.k_proj.bias\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "model.layers.11.self_attn.v_proj.bias\n",
      "model.layers.11.self_attn.dense.weight\n",
      "model.layers.11.self_attn.dense.bias\n",
      "model.layers.11.mlp.fc1.weight\n",
      "model.layers.11.mlp.fc1.bias\n",
      "model.layers.11.mlp.fc2.weight\n",
      "model.layers.11.mlp.fc2.bias\n",
      "model.layers.11.input_layernorm.weight\n",
      "model.layers.11.input_layernorm.bias\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "model.layers.12.self_attn.q_proj.bias\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "model.layers.12.self_attn.k_proj.bias\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "model.layers.12.self_attn.v_proj.bias\n",
      "model.layers.12.self_attn.dense.weight\n",
      "model.layers.12.self_attn.dense.bias\n",
      "model.layers.12.mlp.fc1.weight\n",
      "model.layers.12.mlp.fc1.bias\n",
      "model.layers.12.mlp.fc2.weight\n",
      "model.layers.12.mlp.fc2.bias\n",
      "model.layers.12.input_layernorm.weight\n",
      "model.layers.12.input_layernorm.bias\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "model.layers.13.self_attn.q_proj.bias\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "model.layers.13.self_attn.k_proj.bias\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "model.layers.13.self_attn.v_proj.bias\n",
      "model.layers.13.self_attn.dense.weight\n",
      "model.layers.13.self_attn.dense.bias\n",
      "model.layers.13.mlp.fc1.weight\n",
      "model.layers.13.mlp.fc1.bias\n",
      "model.layers.13.mlp.fc2.weight\n",
      "model.layers.13.mlp.fc2.bias\n",
      "model.layers.13.input_layernorm.weight\n",
      "model.layers.13.input_layernorm.bias\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "model.layers.14.self_attn.q_proj.bias\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "model.layers.14.self_attn.k_proj.bias\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "model.layers.14.self_attn.v_proj.bias\n",
      "model.layers.14.self_attn.dense.weight\n",
      "model.layers.14.self_attn.dense.bias\n",
      "model.layers.14.mlp.fc1.weight\n",
      "model.layers.14.mlp.fc1.bias\n",
      "model.layers.14.mlp.fc2.weight\n",
      "model.layers.14.mlp.fc2.bias\n",
      "model.layers.14.input_layernorm.weight\n",
      "model.layers.14.input_layernorm.bias\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "model.layers.15.self_attn.q_proj.bias\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "model.layers.15.self_attn.k_proj.bias\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "model.layers.15.self_attn.v_proj.bias\n",
      "model.layers.15.self_attn.dense.weight\n",
      "model.layers.15.self_attn.dense.bias\n",
      "model.layers.15.mlp.fc1.weight\n",
      "model.layers.15.mlp.fc1.bias\n",
      "model.layers.15.mlp.fc2.weight\n",
      "model.layers.15.mlp.fc2.bias\n",
      "model.layers.15.input_layernorm.weight\n",
      "model.layers.15.input_layernorm.bias\n",
      "model.layers.16.self_attn.q_proj.weight\n",
      "model.layers.16.self_attn.q_proj.bias\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "model.layers.16.self_attn.k_proj.bias\n",
      "model.layers.16.self_attn.v_proj.weight\n",
      "model.layers.16.self_attn.v_proj.bias\n",
      "model.layers.16.self_attn.dense.weight\n",
      "model.layers.16.self_attn.dense.bias\n",
      "model.layers.16.mlp.fc1.weight\n",
      "model.layers.16.mlp.fc1.bias\n",
      "model.layers.16.mlp.fc2.weight\n",
      "model.layers.16.mlp.fc2.bias\n",
      "model.layers.16.input_layernorm.weight\n",
      "model.layers.16.input_layernorm.bias\n",
      "model.layers.17.self_attn.q_proj.weight\n",
      "model.layers.17.self_attn.q_proj.bias\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "model.layers.17.self_attn.k_proj.bias\n",
      "model.layers.17.self_attn.v_proj.weight\n",
      "model.layers.17.self_attn.v_proj.bias\n",
      "model.layers.17.self_attn.dense.weight\n",
      "model.layers.17.self_attn.dense.bias\n",
      "model.layers.17.mlp.fc1.weight\n",
      "model.layers.17.mlp.fc1.bias\n",
      "model.layers.17.mlp.fc2.weight\n",
      "model.layers.17.mlp.fc2.bias\n",
      "model.layers.17.input_layernorm.weight\n",
      "model.layers.17.input_layernorm.bias\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "model.layers.18.self_attn.q_proj.bias\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "model.layers.18.self_attn.k_proj.bias\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "model.layers.18.self_attn.v_proj.bias\n",
      "model.layers.18.self_attn.dense.weight\n",
      "model.layers.18.self_attn.dense.bias\n",
      "model.layers.18.mlp.fc1.weight\n",
      "model.layers.18.mlp.fc1.bias\n",
      "model.layers.18.mlp.fc2.weight\n",
      "model.layers.18.mlp.fc2.bias\n",
      "model.layers.18.input_layernorm.weight\n",
      "model.layers.18.input_layernorm.bias\n",
      "model.layers.19.self_attn.q_proj.weight\n",
      "model.layers.19.self_attn.q_proj.bias\n",
      "model.layers.19.self_attn.k_proj.weight\n",
      "model.layers.19.self_attn.k_proj.bias\n",
      "model.layers.19.self_attn.v_proj.weight\n",
      "model.layers.19.self_attn.v_proj.bias\n",
      "model.layers.19.self_attn.dense.weight\n",
      "model.layers.19.self_attn.dense.bias\n",
      "model.layers.19.mlp.fc1.weight\n",
      "model.layers.19.mlp.fc1.bias\n",
      "model.layers.19.mlp.fc2.weight\n",
      "model.layers.19.mlp.fc2.bias\n",
      "model.layers.19.input_layernorm.weight\n",
      "model.layers.19.input_layernorm.bias\n",
      "model.layers.20.self_attn.q_proj.weight\n",
      "model.layers.20.self_attn.q_proj.bias\n",
      "model.layers.20.self_attn.k_proj.weight\n",
      "model.layers.20.self_attn.k_proj.bias\n",
      "model.layers.20.self_attn.v_proj.weight\n",
      "model.layers.20.self_attn.v_proj.bias\n",
      "model.layers.20.self_attn.dense.weight\n",
      "model.layers.20.self_attn.dense.bias\n",
      "model.layers.20.mlp.fc1.weight\n",
      "model.layers.20.mlp.fc1.bias\n",
      "model.layers.20.mlp.fc2.weight\n",
      "model.layers.20.mlp.fc2.bias\n",
      "model.layers.20.input_layernorm.weight\n",
      "model.layers.20.input_layernorm.bias\n",
      "model.layers.21.self_attn.q_proj.weight\n",
      "model.layers.21.self_attn.q_proj.bias\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "model.layers.21.self_attn.k_proj.bias\n",
      "model.layers.21.self_attn.v_proj.weight\n",
      "model.layers.21.self_attn.v_proj.bias\n",
      "model.layers.21.self_attn.dense.weight\n",
      "model.layers.21.self_attn.dense.bias\n",
      "model.layers.21.mlp.fc1.weight\n",
      "model.layers.21.mlp.fc1.bias\n",
      "model.layers.21.mlp.fc2.weight\n",
      "model.layers.21.mlp.fc2.bias\n",
      "model.layers.21.input_layernorm.weight\n",
      "model.layers.21.input_layernorm.bias\n",
      "model.layers.22.self_attn.q_proj.weight\n",
      "model.layers.22.self_attn.q_proj.bias\n",
      "model.layers.22.self_attn.k_proj.weight\n",
      "model.layers.22.self_attn.k_proj.bias\n",
      "model.layers.22.self_attn.v_proj.weight\n",
      "model.layers.22.self_attn.v_proj.bias\n",
      "model.layers.22.self_attn.dense.weight\n",
      "model.layers.22.self_attn.dense.bias\n",
      "model.layers.22.mlp.fc1.weight\n",
      "model.layers.22.mlp.fc1.bias\n",
      "model.layers.22.mlp.fc2.weight\n",
      "model.layers.22.mlp.fc2.bias\n",
      "model.layers.22.input_layernorm.weight\n",
      "model.layers.22.input_layernorm.bias\n",
      "model.layers.23.self_attn.q_proj.weight\n",
      "model.layers.23.self_attn.q_proj.bias\n",
      "model.layers.23.self_attn.k_proj.weight\n",
      "model.layers.23.self_attn.k_proj.bias\n",
      "model.layers.23.self_attn.v_proj.weight\n",
      "model.layers.23.self_attn.v_proj.bias\n",
      "model.layers.23.self_attn.dense.weight\n",
      "model.layers.23.self_attn.dense.bias\n",
      "model.layers.23.mlp.fc1.weight\n",
      "model.layers.23.mlp.fc1.bias\n",
      "model.layers.23.mlp.fc2.weight\n",
      "model.layers.23.mlp.fc2.bias\n",
      "model.layers.23.input_layernorm.weight\n",
      "model.layers.23.input_layernorm.bias\n",
      "model.layers.24.self_attn.q_proj.weight\n",
      "model.layers.24.self_attn.q_proj.bias\n",
      "model.layers.24.self_attn.k_proj.weight\n",
      "model.layers.24.self_attn.k_proj.bias\n",
      "model.layers.24.self_attn.v_proj.weight\n",
      "model.layers.24.self_attn.v_proj.bias\n",
      "model.layers.24.self_attn.dense.weight\n",
      "model.layers.24.self_attn.dense.bias\n",
      "model.layers.24.mlp.fc1.weight\n",
      "model.layers.24.mlp.fc1.bias\n",
      "model.layers.24.mlp.fc2.weight\n",
      "model.layers.24.mlp.fc2.bias\n",
      "model.layers.24.input_layernorm.weight\n",
      "model.layers.24.input_layernorm.bias\n",
      "model.layers.25.self_attn.q_proj.weight\n",
      "model.layers.25.self_attn.q_proj.bias\n",
      "model.layers.25.self_attn.k_proj.weight\n",
      "model.layers.25.self_attn.k_proj.bias\n",
      "model.layers.25.self_attn.v_proj.weight\n",
      "model.layers.25.self_attn.v_proj.bias\n",
      "model.layers.25.self_attn.dense.weight\n",
      "model.layers.25.self_attn.dense.bias\n",
      "model.layers.25.mlp.fc1.weight\n",
      "model.layers.25.mlp.fc1.bias\n",
      "model.layers.25.mlp.fc2.weight\n",
      "model.layers.25.mlp.fc2.bias\n",
      "model.layers.25.input_layernorm.weight\n",
      "model.layers.25.input_layernorm.bias\n",
      "model.layers.26.self_attn.q_proj.weight\n",
      "model.layers.26.self_attn.q_proj.bias\n",
      "model.layers.26.self_attn.k_proj.weight\n",
      "model.layers.26.self_attn.k_proj.bias\n",
      "model.layers.26.self_attn.v_proj.weight\n",
      "model.layers.26.self_attn.v_proj.bias\n",
      "model.layers.26.self_attn.dense.weight\n",
      "model.layers.26.self_attn.dense.bias\n",
      "model.layers.26.mlp.fc1.weight\n",
      "model.layers.26.mlp.fc1.bias\n",
      "model.layers.26.mlp.fc2.weight\n",
      "model.layers.26.mlp.fc2.bias\n",
      "model.layers.26.input_layernorm.weight\n",
      "model.layers.26.input_layernorm.bias\n",
      "model.layers.27.self_attn.q_proj.weight\n",
      "model.layers.27.self_attn.q_proj.bias\n",
      "model.layers.27.self_attn.k_proj.weight\n",
      "model.layers.27.self_attn.k_proj.bias\n",
      "model.layers.27.self_attn.v_proj.weight\n",
      "model.layers.27.self_attn.v_proj.bias\n",
      "model.layers.27.self_attn.dense.weight\n",
      "model.layers.27.self_attn.dense.bias\n",
      "model.layers.27.mlp.fc1.weight\n",
      "model.layers.27.mlp.fc1.bias\n",
      "model.layers.27.mlp.fc2.weight\n",
      "model.layers.27.mlp.fc2.bias\n",
      "model.layers.27.input_layernorm.weight\n",
      "model.layers.27.input_layernorm.bias\n",
      "model.layers.28.self_attn.q_proj.weight\n",
      "model.layers.28.self_attn.q_proj.bias\n",
      "model.layers.28.self_attn.k_proj.weight\n",
      "model.layers.28.self_attn.k_proj.bias\n",
      "model.layers.28.self_attn.v_proj.weight\n",
      "model.layers.28.self_attn.v_proj.bias\n",
      "model.layers.28.self_attn.dense.weight\n",
      "model.layers.28.self_attn.dense.bias\n",
      "model.layers.28.mlp.fc1.weight\n",
      "model.layers.28.mlp.fc1.bias\n",
      "model.layers.28.mlp.fc2.weight\n",
      "model.layers.28.mlp.fc2.bias\n",
      "model.layers.28.input_layernorm.weight\n",
      "model.layers.28.input_layernorm.bias\n",
      "model.layers.29.self_attn.q_proj.weight\n",
      "model.layers.29.self_attn.q_proj.bias\n",
      "model.layers.29.self_attn.k_proj.weight\n",
      "model.layers.29.self_attn.k_proj.bias\n",
      "model.layers.29.self_attn.v_proj.weight\n",
      "model.layers.29.self_attn.v_proj.bias\n",
      "model.layers.29.self_attn.dense.weight\n",
      "model.layers.29.self_attn.dense.bias\n",
      "model.layers.29.mlp.fc1.weight\n",
      "model.layers.29.mlp.fc1.bias\n",
      "model.layers.29.mlp.fc2.weight\n",
      "model.layers.29.mlp.fc2.bias\n",
      "model.layers.29.input_layernorm.weight\n",
      "model.layers.29.input_layernorm.bias\n",
      "model.layers.30.self_attn.q_proj.weight\n",
      "model.layers.30.self_attn.q_proj.bias\n",
      "model.layers.30.self_attn.k_proj.weight\n",
      "model.layers.30.self_attn.k_proj.bias\n",
      "model.layers.30.self_attn.v_proj.weight\n",
      "model.layers.30.self_attn.v_proj.bias\n",
      "model.layers.30.self_attn.dense.weight\n",
      "model.layers.30.self_attn.dense.bias\n",
      "model.layers.30.mlp.fc1.weight\n",
      "model.layers.30.mlp.fc1.bias\n",
      "model.layers.30.mlp.fc2.weight\n",
      "model.layers.30.mlp.fc2.bias\n",
      "model.layers.30.input_layernorm.weight\n",
      "model.layers.30.input_layernorm.bias\n",
      "model.layers.31.self_attn.q_proj.weight\n",
      "model.layers.31.self_attn.q_proj.bias\n",
      "model.layers.31.self_attn.k_proj.weight\n",
      "model.layers.31.self_attn.k_proj.bias\n",
      "model.layers.31.self_attn.v_proj.weight\n",
      "model.layers.31.self_attn.v_proj.bias\n",
      "model.layers.31.self_attn.dense.weight\n",
      "model.layers.31.self_attn.dense.bias\n",
      "model.layers.31.mlp.fc1.weight\n",
      "model.layers.31.mlp.fc1.bias\n",
      "model.layers.31.mlp.fc2.weight\n",
      "model.layers.31.mlp.fc2.bias\n",
      "model.layers.31.input_layernorm.weight\n",
      "model.layers.31.input_layernorm.bias\n",
      "model.final_layernorm.weight\n",
      "model.final_layernorm.bias\n",
      "lm_head.weight\n",
      "lm_head.bias\n"
     ]
    }
   ],
   "source": [
    "modules = list(model.state_dict().keys())\n",
    "print(\"\\n\".join(modules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "  (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "  (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "  (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "  (rotary_emb): PhiRotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(get_submodule(model, \"model.layers.1.self_attn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pattern = \"model.layers.*.self_attn.v_proj\"\n",
    "\n",
    "target_modules = {}\n",
    "for module in modules:\n",
    "    match = re.search(target_pattern, module)\n",
    "    if match:\n",
    "        target_module = match.group()\n",
    "        target_modules[target_module] = get_submodule(model, target_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.layers.0.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.1.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.2.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.3.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.4.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.5.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.6.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.7.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.8.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.9.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.10.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.11.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.12.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.13.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.14.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.15.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.16.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.17.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.18.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.19.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.20.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.21.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.22.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.23.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.24.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.25.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.26.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.27.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.28.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.29.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.30.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True),\n",
       " 'model.layers.31.self_attn.v_proj': Linear(in_features=2560, out_features=2560, bias=True)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_outputs = {k: [] for k in target_modules.keys()}\n",
    "\n",
    "def record_output(module, input, output, name):\n",
    "    for i in range(output.shape[0]):\n",
    "        recorded_outputs[name].append(output[i])\n",
    "\n",
    "for name, module in target_modules.items():\n",
    "    module.register_forward_hook(partial(record_output, name=name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_samples = [\n",
    "    \"The dog is on the mat.\",\n",
    "    \"Do you know the muffin man\",\n",
    "    \"Hasta la vista baby\",\n",
    "    \"Sugar spice and everything nice\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer(\n",
    "    input_samples,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  464,  3290,   318,   319,   262,  2603,    13],\n",
       "        [ 5211,   345,   760,   262, 27563,   259,   582],\n",
       "        [   39, 40197,  8591,   410, 12523,  5156, 50256],\n",
       "        [   50, 35652, 25721,   290,  2279,  3621, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"\n",
    "\n",
    "model.to(device).eval()\n",
    "input_tokens = {k: v.to(device) for k, v in input_tokens.items()}\n",
    "\n",
    "\n",
    "model_outputs = model(**input_tokens, output_hidden_states=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.v_proj: 4\n",
      "model.layers.1.self_attn.v_proj: 4\n",
      "model.layers.2.self_attn.v_proj: 4\n",
      "model.layers.3.self_attn.v_proj: 4\n",
      "model.layers.4.self_attn.v_proj: 4\n",
      "model.layers.5.self_attn.v_proj: 4\n",
      "model.layers.6.self_attn.v_proj: 4\n",
      "model.layers.7.self_attn.v_proj: 4\n",
      "model.layers.8.self_attn.v_proj: 4\n",
      "model.layers.9.self_attn.v_proj: 4\n",
      "model.layers.10.self_attn.v_proj: 4\n",
      "model.layers.11.self_attn.v_proj: 4\n",
      "model.layers.12.self_attn.v_proj: 4\n",
      "model.layers.13.self_attn.v_proj: 4\n",
      "model.layers.14.self_attn.v_proj: 4\n",
      "model.layers.15.self_attn.v_proj: 4\n",
      "model.layers.16.self_attn.v_proj: 4\n",
      "model.layers.17.self_attn.v_proj: 4\n",
      "model.layers.18.self_attn.v_proj: 4\n",
      "model.layers.19.self_attn.v_proj: 4\n",
      "model.layers.20.self_attn.v_proj: 4\n",
      "model.layers.21.self_attn.v_proj: 4\n",
      "model.layers.22.self_attn.v_proj: 4\n",
      "model.layers.23.self_attn.v_proj: 4\n",
      "model.layers.24.self_attn.v_proj: 4\n",
      "model.layers.25.self_attn.v_proj: 4\n",
      "model.layers.26.self_attn.v_proj: 4\n",
      "model.layers.27.self_attn.v_proj: 4\n",
      "model.layers.28.self_attn.v_proj: 4\n",
      "model.layers.29.self_attn.v_proj: 4\n",
      "model.layers.30.self_attn.v_proj: 4\n",
      "model.layers.31.self_attn.v_proj: 4\n"
     ]
    }
   ],
   "source": [
    "for name, outs in recorded_outputs.items():\n",
    "    print(f\"{name}: {len(outs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7, 2560])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(recorded_outputs[\"model.layers.13.self_attn.v_proj\"]).shape\n",
    "# n_tokens, n_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def principal_components(data, n_components):\n",
    "    data = data - data.mean(dim=0)\n",
    "    U, S, V = torch.linalg.svd(data)\n",
    "    proj = V[:,:n_components]\n",
    "    return proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
